{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import warnings\n",
    "import nltk, string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd \n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from random import randint\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dict():\n",
    "    train_data = open('train.txt', 'r') \n",
    "    source_list = []\n",
    "    train_classification_data = []\n",
    "    nodes_list = []\n",
    "    source_sink_dict = {}\n",
    "    while True: \n",
    "        sink_list = []\n",
    "        line = train_data.readline()   \n",
    "        if not line:    \n",
    "            break\n",
    "        nodes = re.split(' |\\t',line.strip())\n",
    "        source = nodes[0]\n",
    "        for node in nodes[1:]:\n",
    "            sink_list.append(node)\n",
    "        source_sink_dict[source] = sink_list\n",
    "    train_data.close() \n",
    "    return source_sink_dict\n",
    "\n",
    "def get_test_data():\n",
    "    test_data = pd.read_csv(\"test-public.csv\")\n",
    "    test_data = test_data.drop(columns=['Id'])\n",
    "    test_data.columns = ['source', 'sink']\n",
    "    return test_data\n",
    "\n",
    "def get_node_features():\n",
    "    with open('nodes.json') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def generate_flatten_source_sink_df():\n",
    "    source_sink_dict = get_train_dict()\n",
    "    sink_list = []\n",
    "    source_list = []\n",
    "    keys = list(source_sink_dict.keys())\n",
    "\n",
    "    for key in source_sink_dict.keys():\n",
    "        sinks = source_sink_dict[key]\n",
    "        for sink in sinks:\n",
    "            source_list.append(key)\n",
    "            sink_list.append(sink)\n",
    "    source_list = {'Source': source_list, 'Sink': sink_list}  \n",
    "    \n",
    "    df_source_sink = pd.DataFrame(source_list) \n",
    "    df_source_sink.to_csv('train_source_link_df.csv',header=False,index=False)\n",
    "    \n",
    "    g = nx.read_edgelist('train_source_link_df.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n",
    "    print(\"=\"*70)\n",
    "    print('Information about graph')\n",
    "    print(\"=\"*70)\n",
    "    print(nx.info(g))\n",
    "    return df_source_sink\n",
    "\n",
    "def generate_missing_edges():\n",
    "    generate_flatten_source_sink_df()\n",
    "    df_source_sink = csv.reader(open('train_source_link_df.csv','r'))\n",
    "    edges = dict()\n",
    "    for edge in df_source_sink:\n",
    "        edges[(edge[0], edge[1])] = 1\n",
    "\n",
    "    missing_edges = set([])\n",
    "    while (len(missing_edges) < 53872):\n",
    "        a= random.randint(0, 4084)\n",
    "        b= random.randint(0, 4084)\n",
    "        tmp = edges.get((a,b),-1)\n",
    "        if tmp == -1 and a!=b:\n",
    "            try:\n",
    "                if nx.shortest_path_length(g,source=a,target=b) > 3: \n",
    "                    missing_edges.add((a,b))\n",
    "                else:\n",
    "                    continue  \n",
    "            except:  \n",
    "                missing_edges.add((a,b))              \n",
    "        else:\n",
    "            continue\n",
    "    return missing_edges\n",
    "\n",
    "def create_dataframes():\n",
    "    missing_edges = generate_missing_edges()\n",
    "    print('Length of missing edges:' ,len(missing_edges))\n",
    "    df_true_relationship = pd.read_csv('train_source_link_df.csv', names=[\"Source\", \"Sink\"])\n",
    "    df_false_relationship = pd.DataFrame(list(missing_edges), columns=['Source', 'Sink'])\n",
    "    frames = [df_true_relationship, df_false_relationship]\n",
    "    df_relationship = pd.concat(frames)\n",
    "    true_labels = np.ones(len(df_true_relationship))\n",
    "    false_labels = np.zeros(len(df_false_relationship))\n",
    "    labels = np.concatenate((true_labels, false_labels), axis=0)\n",
    "    print(\"Number of pair of nodes with edges:\", df_true_relationship.shape[0])\n",
    "    print(\"Number of pair of nodes without edges:\", df_false_relationship.shape[0])\n",
    "    df_true_relationship.to_csv('df_true_relationship.csv',header=False,index=False)\n",
    "    print('Total Length:',len(df_relationship))\n",
    "    return df_relationship, labels\n",
    "    \n",
    "def preprocess_keywords(feature):\n",
    "    keywords_list = [x for x in feature.keys() if x.startswith(\"keyword\")]\n",
    "    keywords = ' '.join([str(elem) for elem in keywords_list]) \n",
    "    return keywords\n",
    "\n",
    "def preprocess_venues(feature):\n",
    "    venues_list = [x for x in feature.keys() if x.startswith(\"venue\")]\n",
    "    venues = ' '.join([str(elem) for elem in venues_list]) \n",
    "    return venues\n",
    "\n",
    "def preprocess_years_publications(feature):\n",
    "    first_list = [x for x in feature.keys() if x.startswith(\"first\")]\n",
    "    last_list = [x for x in feature.keys() if x.startswith(\"last\")]\n",
    "    years_pub = int(feature[first_list[0]]) - int(feature[last_list[0]])\n",
    "    return years_pub\n",
    "\n",
    "def preprocess_num_papers(feature):\n",
    "    num_papers_list = [x for x in feature.keys() if x.startswith(\"num_papers\")]\n",
    "    return feature[num_papers_list[0]]\n",
    "\n",
    "def preprocess_first(feature):\n",
    "    first_list = [x for x in feature.keys() if x.startswith(\"first\")]\n",
    "    return feature[first_list[0]]\n",
    "\n",
    "def preprocess_last(feature):\n",
    "    last_list = [x for x in feature.keys() if x.startswith(\"last\")]\n",
    "    return feature[last_list[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFeatureSelectionGraphs(X_train, clf):\n",
    "    print(\"=\"*100)\n",
    "    print('Feature Importance')\n",
    "    print(\"=\"*100)\n",
    "    features = X_train.columns\n",
    "    importances = clf.feature_importances_\n",
    "    indices = (np.argsort(importances))[-25:]\n",
    "    plt.figure(figsize=(6,8))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.show()\n",
    "    print(\"=\"*100)\n",
    "    print('Correlation Matrix')\n",
    "    print(\"=\"*100)\n",
    "    plt.figure(figsize=(14,12))\n",
    "    sns.heatmap(X_train.corr(),\n",
    "            vmax=0.5,\n",
    "            square=True,\n",
    "            annot=True)\n",
    "    \n",
    "def calculate_adar(a,b,G):\n",
    "    sum=0\n",
    "    try:\n",
    "        n=list(set(G.successors(a)).intersection(set(G.successors(b))))\n",
    "        if len(n)!=0:\n",
    "            for i in n:\n",
    "                sum=sum+(1/np.log10(len(list(G.predecessors(i)))))\n",
    "            return sum\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def jaccard_for_outcoming(a,b, G):\n",
    "    try:\n",
    "        if len(set(G.successors(a))) == 0  | len(set(G.successors(b))) == 0:\n",
    "            return 0\n",
    "        sim = (len(set(G.successors(a)).intersection(set(G.successors(b)))))/\\\n",
    "                                    (len(set(G.successors(a)).union(set(G.successors(b)))))\n",
    "    except:\n",
    "        return 0\n",
    "    return sim\n",
    "\n",
    "def jaccard_for_incoming(a,b, G):\n",
    "    try:\n",
    "        if len(set(G.predecessors(a))) == 0  | len(set(G.predecessors(b))) == 0:\n",
    "            return 0\n",
    "        sim = (len(set(G.predecessors(a)).intersection(set(G.predecessors(b)))))/\\\n",
    "                                 (len(set(G.predecessors(a)).union(set(G.predecessors(b)))))\n",
    "        return sim\n",
    "    except:\n",
    "        return 0\n",
    "            \n",
    "def cosine_sim(text1, text2):\n",
    "    if (text1 == text2):\n",
    "        return 1\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "def get_features():\n",
    "    keyword_dict = {}\n",
    "    venue_dict = {}\n",
    "    num_papers_dict = {}\n",
    "    years_publications = {}\n",
    "    first_dict = {}\n",
    "    last_dict = {}\n",
    "    feature_list = get_node_features()\n",
    "    for feature in feature_list:\n",
    "        keyword_dict[feature['id']] = preprocess_keywords(feature)\n",
    "        venue_dict[feature['id']] = preprocess_venues(feature)\n",
    "        num_papers_dict[feature['id']] = preprocess_num_papers(feature)\n",
    "        years_publications[feature['id']] = preprocess_years_publications(feature)\n",
    "        first_dict[feature['id']] = preprocess_first(feature)\n",
    "        last_dict[feature['id']] = preprocess_last(feature)\n",
    "\n",
    "    return keyword_dict, venue_dict, num_papers_dict, years_publications,first_dict,last_dict \n",
    "     \n",
    "def assign_features(setX):  \n",
    "    setX.columns = ['source', 'sink']\n",
    "    \n",
    "    G = nx.read_edgelist('df_true_relationship.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)    \n",
    "    keyword_dict, venue_dict, num_papers_dict, years_publications,first_dict,last_dict = get_features()\n",
    "\n",
    "    print('Assigning keywords similarity ...')\n",
    "    setX['keywords_source'] = setX.apply(lambda row: keyword_dict[int(row.source)],axis=1)\n",
    "    setX['keywords_sink'] = setX.apply(lambda row: keyword_dict[int(row.sink)],axis=1)\n",
    "    setX['keywords_similarity'] = setX.apply(lambda row: cosine_sim(row.keywords_source, row.keywords_sink) ,axis=1)\n",
    "\n",
    "    print('Assigning venues similarity ...')\n",
    "    setX['venues_source'] = setX.apply(lambda row: venue_dict[int(row.source)],axis=1)\n",
    "    setX['venues_sink'] = setX.apply(lambda row: venue_dict[int(row.sink)],axis=1)\n",
    "    setX['venues_similarity'] = setX.apply(lambda row: cosine_sim(row.venues_source, row.venues_sink) ,axis=1)\n",
    "\n",
    "    print('Assigning number of papers ...')\n",
    "    setX['num_papers_source'] = setX.apply(lambda row: num_papers_dict[int(row.source)],axis=1)\n",
    "    setX['num_papers_sink'] = setX.apply(lambda row: num_papers_dict[int(row.sink)],axis=1)\n",
    "\n",
    "    print('Assigning active years of publication by authors ...')\n",
    "    setX['years_pub_source'] = setX.apply(lambda row: years_publications[int(row.source)],axis=1)\n",
    "    setX['years_pub_sink'] = setX.apply(lambda row: years_publications[int(row.sink)],axis=1)\n",
    "    \n",
    "    print('Assigning in/out degree centrality ...')\n",
    "    out_degree_centrality = nx.out_degree_centrality(G)\n",
    "    in_degree_centrality = nx.in_degree_centrality(G)\n",
    "    setX['source_out_centrality'] = setX.apply(lambda row: out_degree_centrality[int(row.source)] if int(row.source) in out_degree_centrality else 0,axis=1)\n",
    "    setX['sink_in_centrality'] = setX.apply(lambda row: in_degree_centrality[int(row.sink)] if int(row.sink) in in_degree_centrality else 0,axis=1)\n",
    "    \n",
    "    print('Assigning page rank ...')\n",
    "    page_rank = nx.pagerank_scipy(G)\n",
    "    setX['source_page_rank'] = setX.apply(lambda row: page_rank[int(row.source)] if int(row.source) in page_rank else 0 ,axis=1)\n",
    "    setX['sink_page_rank'] = setX.apply(lambda row: page_rank[int(row.sink)] if int(row.sink) in page_rank else 0,axis=1)\n",
    "\n",
    "    print('Assigning preferential attachment ...')\n",
    "    setX['preferential_attachment'] = setX.apply(lambda row: row.source_out_centrality * row.sink_in_centrality,axis=1)\n",
    "\n",
    "    print('Assigning hub and authority score ...')\n",
    "    hub_score, authority_score = nx.hits(G)\n",
    "    setX['source_hub_score'] = setX.apply(lambda row: hub_score[int(row.source)] if int(row.source) in hub_score else 0,axis=1)\n",
    "    setX['sink_hub_score'] = setX.apply(lambda row: hub_score[int(row.sink)] if int(row.sink) in hub_score else 0,axis=1)\n",
    "    setX['source_authority_score'] = setX.apply(lambda row: authority_score[int(row.source)] if int(row.source) in authority_score else 0,axis=1)\n",
    "    setX['sink_authority_score'] = setX.apply(lambda row: authority_score[int(row.sink)] if int(row.sink) in authority_score else 0,axis=1)\n",
    "\n",
    "    print('Assigning katz ...')\n",
    "    katz = nx.katz.katz_centrality(G,alpha=0.005,beta=1)\n",
    "    setX['katz_source'] = setX.apply(lambda row: katz[int(row.source)] if int(row.source) in katz else 0, axis =1)\n",
    "    setX['katz_sink'] = setX.apply(lambda row: katz[int(row.sink)] if int(row.sink) in katz else 0, axis =1)\n",
    "    \n",
    "    print('Assigning adar ...')\n",
    "    setX['adar'] = setX.apply(lambda row: calculate_adar(row.source, row.sink, G) ,axis=1)\n",
    "    \n",
    "    print('Assigning jaccard ...')\n",
    "    setX['jaccard_for_outcoming'] = setX.apply(lambda row: jaccard_for_outcoming(row.source, row.sink, G) ,axis=1)\n",
    "    setX['jaccard_for_incoming'] = setX.apply(lambda row: jaccard_for_incoming(row.source, row.sink, G) ,axis=1)\n",
    "    \n",
    "    print('Assigning number of neighbors ...')\n",
    "    setX['source_neighbours'] = setX.apply(lambda row: len(list(G.neighbors(int(row.source)))) if int(row.source) in G else 0 ,axis=1)\n",
    "    setX['sink_neighbours'] = setX.apply(lambda row: len(list(G.neighbors(int(row.sink)))) if int(row.sink) in G else 0 ,axis=1)\n",
    "\n",
    "    return setX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_comparison(X ,y):\n",
    "    print(\"=\"*100)\n",
    "    print('Algorithms Comparison')\n",
    "    print(\"=\"*100)\n",
    "    seed = 7\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('DT', DecisionTreeClassifier()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC()))\n",
    "    values = []\n",
    "    names = []\n",
    "    scoring = 'accuracy'\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "        values.append(cv_results.mean())\n",
    "        names.append(name)\n",
    "        results = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(results)\n",
    "   \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.bar(names, values)\n",
    "    plt.subplot(133)\n",
    "    plt.plot(names, values)\n",
    "    plt.suptitle('Accuracy Comparison')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_best_parameters(X, y):\n",
    "    print(\"=\"*100)\n",
    "    print('Best parameters RF and LR')\n",
    "    print(\"=\"*100)\n",
    "    param_dist = {\"n_estimators\":sp_randint(105,125),\n",
    "              \"max_depth\": sp_randint(10,20),\n",
    "              \"min_samples_split\": sp_randint(10,100),\n",
    "              \"min_samples_leaf\": sp_randint(20,60)}\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=25,n_jobs=-1)\n",
    "    rf_random = RandomizedSearchCV(clf, param_distributions=param_dist,n_iter=5,cv=10,scoring='f1',random_state=25)\n",
    "    rf_random.fit(X,y)\n",
    "    print('Random Forest')\n",
    "    print(rf_random.best_params_)\n",
    "    \n",
    "    logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200, random_state=0)\n",
    "    distributions = dict(C=uniform(loc=0, scale=4),penalty=['l2', 'l1'])\n",
    "    clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "    search = clf.fit(X, y)\n",
    "    print('Logistic Regression')\n",
    "    print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Information about graph\n",
      "======================================================================\n",
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 4016\n",
      "Number of edges: 53872\n",
      "Average in degree:  13.4143\n",
      "Average out degree:  13.4143\n",
      "Length of missing edges: 53872\n",
      "Number of pair of nodes with edges: 53872\n",
      "Number of pair of nodes without edges: 53872\n",
      "Total Length: 107744\n",
      "====================================================================================================\n",
      "Feature Engineering for training data\n",
      "====================================================================================================\n",
      "Assigning features for train\n",
      "Assigning keywords similarity ...\n",
      "Assigning venues similarity ...\n",
      "Assigning number of papers ...\n",
      "Assigning active years of publication by authors ...\n",
      "Assigning in/out degree centrality ...\n",
      "Assigning page rank ...\n",
      "Assigning preferential attachment ...\n",
      "Assigning hub and authority score ...\n",
      "Assigning katz ...\n",
      "Assigning adar ...\n",
      "Assigning jaccard ...\n",
      "Assigning number of neighbors ...\n",
      "Features were assigned for training data!\n",
      "====================================================================================================\n",
      "Results with splitted test data\n",
      "====================================================================================================\n",
      "Test accuracy 0.9627824957074574\n",
      "Test f1 score 0.962721948498652\n",
      "====================================================================================================\n",
      "Feature Engineering for testing data\n",
      "====================================================================================================\n",
      "Assigning features for test ...\n",
      "Assigning keywords similarity ...\n",
      "Assigning venues similarity ...\n",
      "Assigning number of papers ...\n",
      "Assigning active years of publication by authors ...\n",
      "Assigning in/out degree centrality ...\n",
      "Assigning page rank ...\n",
      "Assigning preferential attachment ...\n",
      "Assigning hub and authority score ...\n",
      "Assigning katz ...\n",
      "Assigning adar ...\n",
      "Assigning jaccard ...\n",
      "Assigning number of neighbors ...\n",
      "* File for submission created successfully! *\n"
     ]
    }
   ],
   "source": [
    "def make_prediction(): \n",
    "    X, y = create_dataframes()\n",
    "    print(\"=\"*100)\n",
    "    print('Feature Engineering for training data')\n",
    "    print(\"=\"*100)\n",
    "    print('Assigning features for train')\n",
    "\n",
    "    X = assign_features(X)\n",
    "    X = X.drop(columns=['source','sink','keywords_source', 'keywords_sink','venues_source', 'venues_sink'])\n",
    "\n",
    "    print('Features were assigned for training data!')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=9)\n",
    "    \n",
    "    #algorithm_comparison(X, y)\n",
    "    #check_best_parameters(X_train, y_train)\n",
    "   \n",
    "    clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=14, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=23, min_samples_split=11,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=121, n_jobs=-1,\n",
    "            oob_score=False, random_state=25, verbose=0, warm_start=False)\n",
    " \n",
    "    clf.fit(X_train,y_train)\n",
    " \n",
    "    print(\"=\"*100)\n",
    "    print('Results with splitted test data')\n",
    "    print(\"=\"*100)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    print('Test accuracy',accuracy_score(y_test, y_test_pred))\n",
    "    print('Test f1 score',f1_score(y_test,y_test_pred))\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print('Feature Engineering for testing data')\n",
    "    print(\"=\"*100)\n",
    "    print('Assigning features for test ...')\n",
    "\n",
    "    submission_test = get_test_data()\n",
    "    submission_test = assign_features(submission_test)\n",
    "    submission_test = submission_test.drop(columns=['source','sink','keywords_source', 'keywords_sink','venues_source', 'venues_sink'])\n",
    "    sub_test_prob = clf.predict_proba(submission_test)\n",
    "    predictions_df = pd.DataFrame(sub_test_prob)\n",
    "    predictions_df.to_csv('predictions_prob.csv')\n",
    "    print('* File for submission created successfully! *')\n",
    "    \n",
    "    #showFeatureSelectionGraphs(X_train, clf)\n",
    "    \n",
    "make_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
